author : Swapnil Narwade
Date : 16/11/21 
Class : TYBCA 
---------------------------------------

1. Cost function of neural network is non-convex ? 
  - The cost function of a neural network is in general neither convex nor concave. This means that the matrix of all second partial         
    derivatives (the Hessian) is neither positive semidefinite, nor negative semidefinite. Since the second derivative is a matrix, it's         
    possible that it's neither one or the other.

  - To make this analogous to one-variable functions, one could say that the cost function is neither shaped like the graph of x2 nor         
    like the graph of −x2. Another example of a non-convex, non-concave function is sin(x) on R. One of the most striking         
    differences is that ±x2 has only one extremum, whereas sin has infinitely many maxima and minima.

  - How does this relate to our neural network? A cost function J(W,b) has also a number of local maxima and minima, as you can         
    see in this picture, for example. http://www.holehouse.org/mlclass/09_Neural_Networks_Learning_files/Image%20%5B35%5D.png

  - The fact that J has multiple minima can also be interpreted in a nice way. In each layer, you use multiple nodes which are         
    assigned different parameters to make the cost function small. Except for the values of the parameters, these nodes are the same. 
    So you could exchange the parameters of the first node in one layer with those of the second node in the same layer, and accounting for this change 
    in the subsequent layers. You'd end up with a different set of parameters, but the value of the cost function can't be distinguished by 
    (basically you just moved a node, to another place, but kept all the inputs/outputs the same).
    
